{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new RDD from the text file\n",
    "textFile = sc.textFile('../data/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "../data/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # of items in this RDD\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to the Spark documentation!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first item in this RDD\n",
    "# == textFile.take(1) \n",
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome to the Spark documentation!',\n",
       " '',\n",
       " 'This readme will walk you through navigating and building the Spark documentation, which is included here with the Spark source code. You can also find documentation specific to release versions of Spark at https://spark.apache.org/documentation.html.',\n",
       " '',\n",
       " 'Read on to learn more about viewing documentation in plain text (i.e., markdown) or building the documentation yourself. Why build it yourself? So that you have the docs that correspond to whichever version of Spark you currently have checked out of revision control.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list returns first n elements in this RDD.\n",
    "textFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[5] at RDD at PythonRDD.scala:49\n",
      "\n",
      "['Welcome to the Spark documentation!', 'This readme will walk you through navigating and building the Spark documentation, which is included here with the Spark source code. You can also find documentation specific to release versions of Spark at https://spark.apache.org/documentation.html.', 'Read on to learn more about viewing documentation in plain text (i.e., markdown) or building the documentation yourself. Why build it yourself? So that you have the docs that correspond to whichever version of Spark you currently have checked out of revision control.', 'The Spark documentation build uses a number of tools to build HTML docs and API docs in Scala, Java, Python, R and SQL.', 'Note: Other versions of roxygen2 might work in SparkR documentation generation but RoxygenNote field in $SPARK_HOME/R/pkg/DESCRIPTION is 5.0.1, which is updated if the version is mismatched.', 'We include the Spark documentation as part of the source (as opposed to using a hosted wiki, such as the github wiki, as the definitive documentation) to enable the documentation to evolve along with the source code and be captured by revision control (currently git). This way the code automatically includes the version of the documentation that is relevant regardless of which version or release you have checked out or downloaded.', 'You can build just the Spark scaladoc and javadoc by running build/sbt unidoc from the $SPARK_HOME directory.', 'Similarly, you can build just the PySpark docs by running make html from the $SPARK_HOME/python/docs directory. Documentation is only generated for classes that are listed as public in __init__.py. The SparkR docs can be built by running $SPARK_HOME/R/create-docs.sh, and the SQL docs can be built by running $SPARK_HOME/sql/create-docs.sh after building Spark first.', \"When you run jekyll build in the docs directory, it will also copy over the scaladoc and javadoc for the various Spark subprojects into the docs directory (and then also into the _site directory). We use a jekyll plugin to run build/sbt unidoc before building the site so if you haven't run it (recently) it may take some time as it generates all of the scaladoc and javadoc using Unidoc. The jekyll plugin also generates the PySpark docs using Sphinx, SparkR docs using roxygen2 and SQL docs using MkDocs.\"]\n"
     ]
    }
   ],
   "source": [
    "# return a new RDD with a subset of items in RDD \n",
    "# The subset is comprimised of items make user defined fuction return TRUE \n",
    "linesWithSpark = textFile.filter(lambda line: \"Spark\" in line)\n",
    "print(linesWithSpark)\n",
    "print('')\n",
    "# return all the elements of the RDD (dataset) as an array\n",
    "print(linesWithSpark.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# textFile.filter(lambda line: \"Spark\" in line).count()\n",
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map(); r\n",
    "# return a new distributed dataset formed by passing each element of the source through a function func.\n",
    "textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max(a, b):\n",
    "    if a > b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "    \n",
    "textFile.map(lambda line: len(line.split())).reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "214\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "# user defined func should be commutative and associative ==  deterministic\n",
    "# Else, illustrate the race conditon case \n",
    "\n",
    "# textFile func provides an optional second argument for controlling the number of partitions of the file\n",
    "# By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS)\n",
    "textFile = sc.textFile('../data/README.md')\n",
    "print(textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a - b))\n",
    "\n",
    "textFile = sc.textFile('../data/README.md', 3)\n",
    "print(textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a - b))\n",
    "\n",
    "textFile = sc.textFile('../data/README.md', 5)\n",
    "print(textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy transformations\n",
    "\n",
    "\"\"\"RDDs support two types of operations: \n",
    "transformations, which create a new dataset from an existing one, and \n",
    "actions, which return a value to the driver program after running a computation on the dataset. \n",
    "\n",
    "For example, map is a transformation that passes each dataset element through a function \n",
    "    and returns a new RDD representing the results. \n",
    "On the other hand, reduce is an action that aggregates all the elements of the RDD using some function \n",
    "    and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
    "\n",
    "All transformations in Spark are lazy, in that they do not compute their results right away. \n",
    "Instead, they just remember the transformations applied to some base dataset (e.g. a file). \n",
    "The transformations are only computed when an action requires a result to be returned to the driver program. \n",
    "This design enables Spark to run more efficiently. \n",
    "\n",
    "For example, we can realize that a dataset created through map will be used in a reduce \n",
    "    and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
    "\n",
    "By default, each transformed RDD may be recomputed each time you run an action on it. \n",
    "However, you may also persist an RDD in memory using the persist (or cache) method\n",
    "    , in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. \n",
    "    There is also support for persisting RDDs on disk, or replicated across multiple nodes.\"\"\"\n",
    "\n",
    "import math\n",
    "# gives a list\n",
    "# Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing collection in your driver program (a Scala Seq). \n",
    "# The elements of the collection are copied to form a distributed dataset that can be operated on in parallel\n",
    "a = sc.parallelize(range(1, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster than below\n",
    "# transformation is lazy. It doesn't compute the result right away\n",
    "b = a.map(lambda x: math.sqrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slower than above\n",
    "# action requires a result, then compute transformation and action\n",
    "# this tactic can reduce intermediate data\n",
    "b.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf you are using EMR , there are three things\\n\\nusing pyspark(or spark-shell)\\nusing spark-submit without using --master and --deploy-mode\\nusing spark-submit and using --master and --deploy-mode\\nalthough using all the above three will run the application in spark cluster, \\nthere is a difference how the driver program works.\\n\\nin 1st and 2nd the driver will be in client mode \\nwhereas in 3rd the driver will also be in the cluster.\\n\\nin 1st and 2nd, you will have to wait untill one application complete to run another\\n, but in 3rd you can run multiple applications in parallel.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for self contained applications section\n",
    "# for $spark-submit\n",
    "# not $pyspark\n",
    "\"\"\"\n",
    "If you built a spark application, \n",
    "you need to use spark-submit to run the application\n",
    "The code can be written either in python/scala\n",
    "The mode can be either local/cluster\n",
    "\n",
    "If you just want to test/run few individual commands, \n",
    "you can use the shell provided by spark\n",
    "\n",
    "pyspark (for spark in python)\n",
    "spark-shell (for spark in scala)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "If you are using EMR , there are three things\n",
    "\n",
    "using pyspark(or spark-shell)\n",
    "using spark-submit without using --master and --deploy-mode\n",
    "using spark-submit and using --master and --deploy-mode\n",
    "although using all the above three will run the application in spark cluster, \n",
    "there is a difference how the driver program works.\n",
    "\n",
    "in 1st and 2nd the driver will be in client mode \n",
    "whereas in 3rd the driver will also be in the cluster.\n",
    "\n",
    "in 1st and 2nd, you will have to wait untill one application complete to run another\n",
    ", but in 3rd you can run multiple applications in parallel.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
