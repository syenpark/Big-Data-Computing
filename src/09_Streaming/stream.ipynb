{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:08:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:08:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:09:00\n",
      "-------------------------------------------\n",
      "a a a a a\n",
      "b b a a a\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:09:00\n",
      "-------------------------------------------\n",
      "(u'a', 8)\n",
      "(u'b', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:09:05\n",
      "-------------------------------------------\n",
      "a a a aa aa\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:09:05\n",
      "-------------------------------------------\n",
      "(u'a', 3)\n",
      "(u'aa', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:09:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:09:10\n",
      "-------------------------------------------\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Create a DStream that will connect to localhost at port 9999\n",
    "# Start Netcat server: nc -lk 9999 \n",
    "lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "lines.pprint()\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print \"Start\"\n",
    "ssc.awaitTermination(20)  # Wait for the computation to terminate\n",
    "ssc.stop(stopSparkContext=False)  # Stop the StreamingContext without stopping the SparkContext\n",
    "\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:26:00\n",
      "-------------------------------------------\n",
      "(u'other', 15501)\n",
      "(u'first', 10648)\n",
      "(u'many', 9729)\n",
      "(u'new', 6344)\n",
      "(u'system', 5135)\n",
      "(u'american', 4718)\n",
      "(u'several', 4542)\n",
      "(u'=', 4442)\n",
      "(u'same', 4418)\n",
      "(u'century', 4418)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:26:05\n",
      "-------------------------------------------\n",
      "(u'other', 15128)\n",
      "(u'first', 10625)\n",
      "(u'many', 9593)\n",
      "(u'new', 6201)\n",
      "(u'system', 5091)\n",
      "(u'american', 4838)\n",
      "(u'century', 4571)\n",
      "(u'several', 4502)\n",
      "(u'=', 4410)\n",
      "(u'same', 4329)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:26:10\n",
      "-------------------------------------------\n",
      "(u'other', 15236)\n",
      "(u'first', 10532)\n",
      "(u'many', 9711)\n",
      "(u'new', 6186)\n",
      "(u'system', 5209)\n",
      "(u'american', 4898)\n",
      "(u'several', 4596)\n",
      "(u'century', 4517)\n",
      "(u'same', 4285)\n",
      "(u'early', 4253)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:26:15\n",
      "-------------------------------------------\n",
      "(u'other', 15387)\n",
      "(u'first', 10567)\n",
      "(u'many', 9687)\n",
      "(u'new', 6130)\n",
      "(u'system', 5129)\n",
      "(u'american', 4873)\n",
      "(u'several', 4604)\n",
      "(u'century', 4451)\n",
      "(u'=', 4353)\n",
      "(u'same', 4323)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:26:20\n",
      "-------------------------------------------\n",
      "(u'other', 15186)\n",
      "(u'first', 10749)\n",
      "(u'many', 9885)\n",
      "(u'new', 6335)\n",
      "(u'system', 5071)\n",
      "(u'american', 4787)\n",
      "(u'century', 4524)\n",
      "(u'several', 4473)\n",
      "(u'=', 4413)\n",
      "(u'same', 4347)\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "\n",
    "# split the rdd into 5 equal-size parts\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Do word-counting as before\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Use transform() to access any rdd transformations not directly available in SparkStreaming\n",
    "topWords = wordCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "topWords.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(25)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:34:45\n",
      "-------------------------------------------\n",
      "(7560.0, u'great')\n",
      "(6144.0, u'popular')\n",
      "(5487.0, u'best')\n",
      "(4869.0, u'good')\n",
      "(4136.0, u'important')\n",
      "(2284.0, u'strong')\n",
      "(2268.0, u'greater')\n",
      "(2115.0, u'successful')\n",
      "(1854.0, u'novel')\n",
      "(1660.0, u'natural')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:34:50\n",
      "-------------------------------------------\n",
      "(7989.0, u'great')\n",
      "(6081.0, u'popular')\n",
      "(5517.0, u'best')\n",
      "(4668.0, u'good')\n",
      "(4252.0, u'important')\n",
      "(2250.0, u'strong')\n",
      "(2190.0, u'greater')\n",
      "(2061.0, u'successful')\n",
      "(1964.0, u'novel')\n",
      "(1839.0, u'natural')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:34:55\n",
      "-------------------------------------------\n",
      "(7740.0, u'great')\n",
      "(6090.0, u'popular')\n",
      "(5424.0, u'best')\n",
      "(4758.0, u'good')\n",
      "(4212.0, u'important')\n",
      "(2428.0, u'strong')\n",
      "(2313.0, u'greater')\n",
      "(1959.0, u'successful')\n",
      "(1934.0, u'novel')\n",
      "(1851.0, u'greatest')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:35:00\n",
      "-------------------------------------------\n",
      "(7566.0, u'great')\n",
      "(5907.0, u'popular')\n",
      "(5505.0, u'best')\n",
      "(4554.0, u'good')\n",
      "(4240.0, u'important')\n",
      "(2289.0, u'greater')\n",
      "(2276.0, u'strong')\n",
      "(1995.0, u'successful')\n",
      "(1880.0, u'novel')\n",
      "(1781.0, u'natural')\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-11-12 20:35:05\n",
      "-------------------------------------------\n",
      "(7848.0, u'great')\n",
      "(5934.0, u'popular')\n",
      "(5661.0, u'best')\n",
      "(4458.0, u'good')\n",
      "(4296.0, u'important')\n",
      "(2356.0, u'strong')\n",
      "(2307.0, u'greater')\n",
      "(1944.0, u'novel')\n",
      "(1914.0, u'successful')\n",
      "(1869.0, u'natural')\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Find the most positive words in windows of 5 seconds from streaming data\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = sc.textFile(\"../data/AFINN-111.txt\") \\\n",
    "                    .map(parse_line).cache()\n",
    "    \n",
    "ssc = StreamingContext(sc, 5)\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Determine the words with the highest sentiment values by joining the streaming RDD\n",
    "# with the static RDD inside the transform() method and then multiplying\n",
    "# the frequency of the words by its sentiment value\n",
    "happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n",
    "                            .map(lambda (word, tuple):\n",
    "                                 (tuple[0] * tuple[1], word)) \\\n",
    "                            .transform(lambda rdd: rdd.sortByKey(False))\n",
    "\n",
    "happiest_words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(25)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  51396\n",
      "[(u'other', 7673), (u'first', 5348), (u'many', 4890), (u'new', 3206), (u'system', 2596)]\n",
      "refinery: 2\n",
      "Total distinct words:  76869\n",
      "[(u'other', 15501), (u'first', 10648), (u'many', 9729), (u'new', 6344), (u'system', 5135)]\n",
      "refinery: 7\n",
      "Total distinct words:  97070\n",
      "[(u'other', 22994), (u'first', 15880), (u'many', 14547), (u'new', 9468), (u'system', 7743)]\n",
      "refinery: 11\n",
      "Total distinct words:  114682\n",
      "[(u'other', 30629), (u'first', 21273), (u'many', 19322), (u'new', 12545), (u'system', 10226)]\n",
      "refinery: 14\n",
      "Total distinct words:  130487\n",
      "[(u'other', 38350), (u'first', 26539), (u'many', 24270), (u'new', 15619), (u'system', 12858)]\n",
      "refinery: 18\n",
      "Total distinct words:  144782\n",
      "[(u'other', 45865), (u'first', 31805), (u'many', 29033), (u'new', 18731), (u'system', 15435)]\n",
      "refinery: 22\n",
      "Total distinct words:  158283\n",
      "[(u'other', 53546), (u'first', 37076), (u'many', 33895), (u'new', 21750), (u'system', 17975)]\n",
      "refinery: 25\n",
      "Total distinct words:  170708\n",
      "[(u'other', 61252), (u'first', 42372), (u'many', 38720), (u'new', 24861), (u'system', 20564)]\n",
      "refinery: 27\n",
      "Total distinct words:  182405\n",
      "[(u'other', 68883), (u'first', 47746), (u'many', 43628), (u'new', 28014), (u'system', 23071)]\n",
      "refinery: 36\n",
      "Total distinct words:  193450\n",
      "[(u'other', 76438), (u'first', 53121), (u'many', 48605), (u'new', 31196), (u'system', 25635)]\n",
      "refinery: 40\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Stateful word count\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    print \"Total distinct words: \", rdd.count()\n",
    "    print rdd.take(5)\n",
    "    print 'refinery:', rdd.lookup('refinery')[0]\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  51396\n",
      "[(u'other', 7673, 7673), (u'first', 5348, 5348), (u'many', 4890, 4890), (u'new', 3206, 3206), (u'system', 2596, 2596)]\n",
      "refinery: 2 , 2\n",
      "Next threshold =  5\n",
      "Total distinct words:  13974\n",
      "[(u'other', 15496, 15501), (u'first', 10643, 10648), (u'many', 9724, 9729), (u'new', 6339, 6344), (u'system', 5130, 5135)]\n",
      "refinery: 2 , 7\n",
      "Next threshold =  5\n",
      "Total distinct words:  12157\n",
      "[(u'other', 22984, 22994), (u'first', 15870, 15880), (u'many', 14537, 14547), (u'new', 9458, 9468), (u'system', 7733, 7743)]\n",
      "refinery: 1 , 11\n",
      "Next threshold =  4\n",
      "Total distinct words:  12389\n",
      "[(u'other', 30615, 30629), (u'first', 21259, 21273), (u'many', 19308, 19322), (u'new', 12531, 12545), (u'system', 10212, 10226)]\n",
      "refinery: 0 , 14\n",
      "Next threshold =  5\n",
      "Total distinct words:  11657\n",
      "[(u'other', 38331, 38350), (u'first', 26520, 26539), (u'many', 24251, 24270), (u'new', 15600, 15619), (u'system', 12839, 12858)]\n",
      "refinery: 0 , 19\n",
      "Next threshold =  5\n",
      "Total distinct words:  11364\n",
      "[(u'other', 45841, 45865), (u'first', 31781, 31805), (u'many', 29009, 29033), (u'new', 18707, 18731), (u'system', 15411, 15435)]\n",
      "refinery: 0 , 24\n",
      "Next threshold =  5\n",
      "Total distinct words:  11245\n",
      "[(u'other', 53517, 53546), (u'first', 37047, 37076), (u'many', 33866, 33895), (u'new', 21721, 21750), (u'system', 17946, 17975)]\n",
      "refinery: 0 , 29\n",
      "Next threshold =  4\n",
      "Total distinct words:  11920\n",
      "[(u'other', 61219, 61252), (u'first', 42339, 42372), (u'many', 38687, 38720), (u'new', 24828, 24861), (u'system', 20531, 20564)]\n",
      "refinery: 0 , 33\n",
      "Next threshold =  5\n",
      "Total distinct words:  11412\n",
      "[(u'other', 68845, 68883), (u'first', 47708, 47746), (u'many', 43590, 43628), (u'new', 27976, 28014), (u'system', 23033, 23071)]\n",
      "refinery: 4 , 42\n",
      "Next threshold =  5\n",
      "Total distinct words:  11229\n",
      "[(u'other', 76395, 76438), (u'first', 53078, 53121), (u'many', 48562, 48605), (u'new', 31153, 31196), (u'system', 25592, 25635)]\n",
      "refinery: 3 , 46\n",
      "Next threshold =  5\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# MG algorithm for approximate word count\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "k = 10000\n",
    "threshold = 0\n",
    "total_decrement = 0\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    newValue = sum(newValues, runningCount) - threshold\n",
    "    return newValue if newValue > 0 else None\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "            \n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    global threshold, total_decrement \n",
    "    rdd.cache()\n",
    "    print \"Total distinct words: \", rdd.count()\n",
    "    print rdd.map(lambda x: (x[0], x[1], x[1]+total_decrement)).take(5)\n",
    "    lower_bound = rdd.lookup('refinery')\n",
    "    if len(lower_bound) > 0:\n",
    "        lower_bound = lower_bound[0]\n",
    "    else:\n",
    "        lower_bound = 0\n",
    "    print 'refinery:', lower_bound, ',', lower_bound + total_decrement\n",
    "    if rdd.count() > k:\n",
    "        threshold = rdd.zipWithIndex().map(lambda x: (x[1], x[0])).lookup(k)[0][1]\n",
    "    else:\n",
    "        threhold = 0\n",
    "    print \"Next threshold = \", threshold\n",
    "    total_decrement += threshold\n",
    "    rdd.unpersist()\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-11-13 21:41:03\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-11-13 21:41:06\n",
      "-------------------------------------------\n",
      "5\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-11-13 21:41:09\n",
      "-------------------------------------------\n",
      "15\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-11-13 21:41:12\n",
      "-------------------------------------------\n",
      "30\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-11-13 21:41:15\n",
      "-------------------------------------------\n",
      "45\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-11-13 21:41:18\n",
      "-------------------------------------------\n",
      "35\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-11-13 21:41:21\n",
      "-------------------------------------------\n",
      "20\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-11-13 21:41:24\n",
      "-------------------------------------------\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rddQueue = []\n",
    "for i in range(5):\n",
    "    rdd = sc.parallelize([i, i, i, i, i])\n",
    "    rddQueue += [rdd]\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 3 seconds\n",
    "ssc = StreamingContext(sc, 3)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "nums = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Compute the sum over a sliding window of 9 seconds for every 3 seconds\n",
    "# slidingSum = nums.reduceByWindow(lambda x, y: x + y, None, 9, 3)\n",
    "slidingSum = nums.reduceByWindow(lambda x, y: x + y, lambda x, y: x - y, 9, 3)\n",
    "\n",
    "slidingSum.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(24)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "word_counts = words.groupBy('word').count()\n",
    "\n",
    "# Start running the query \n",
    "query = word_counts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "long_words = words.filter(length(words['word'])>=3)\n",
    "\n",
    "# Start running the query \n",
    "query = long_words\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 seconds\", \"5 seconds\"),\n",
    "    words.word)\\\n",
    "    .count()\n",
    "\n",
    "# Start running the query \n",
    "query = windowedCounts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.14\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
