{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/sales.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Find all distinct countries.\n",
    "\n",
    "Hint: use select(), distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      Country|\n",
      "+-------------+\n",
      "|       Sweden|\n",
      "|       Jersey|\n",
      "|     Malaysia|\n",
      "|       Turkey|\n",
      "|      Germany|\n",
      "|       France|\n",
      "|      Belgium|\n",
      "|      Finland|\n",
      "|United States|\n",
      "|        India|\n",
      "|       Kuwait|\n",
      "|        Malta|\n",
      "|        Italy|\n",
      "|       Norway|\n",
      "|        Spain|\n",
      "|      Denmark|\n",
      "|      Ireland|\n",
      "|       Israel|\n",
      "|      Iceland|\n",
      "|  South Korea|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Country\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      Country|\n",
      "+-------------+\n",
      "|       Sweden|\n",
      "|       Jersey|\n",
      "|     Malaysia|\n",
      "|       Turkey|\n",
      "|      Germany|\n",
      "|       France|\n",
      "|      Belgium|\n",
      "|      Finland|\n",
      "|United States|\n",
      "|        India|\n",
      "|       Kuwait|\n",
      "|        Malta|\n",
      "|        Italy|\n",
      "|       Norway|\n",
      "|        Spain|\n",
      "|      Denmark|\n",
      "|      Ireland|\n",
      "|       Israel|\n",
      "|      Iceland|\n",
      "|  South Korea|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# should be same to this one\n",
    "distinct_countries = spark.sql(\"SELECT DISTINCT Country from sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Find the Name and Price of sales records in Brazil.\n",
    "\n",
    "Hint: use filter()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Price|\n",
      "+-------+-----+\n",
      "|Joachim| 1200|\n",
      "|  Diana| 7500|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Country']=='Brazil').select(df['Name'], df['Price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 For each country, find the total Price.\n",
    "\n",
    "Hint: Use groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|      Country|sum(Price)|\n",
      "+-------------+----------+\n",
      "|       Sweden|      8400|\n",
      "|       Jersey|      1200|\n",
      "|     Malaysia|      1200|\n",
      "|       Turkey|      2400|\n",
      "|      Germany|     22800|\n",
      "|       France|     30300|\n",
      "|      Belgium|      3600|\n",
      "|      Finland|      1200|\n",
      "|United States|    350350|\n",
      "|        India|      2400|\n",
      "|       Kuwait|      1200|\n",
      "|        Malta|      3600|\n",
      "|        Italy|      2400|\n",
      "|       Norway|     12000|\n",
      "|        Spain|      2400|\n",
      "|      Denmark|      8400|\n",
      "|      Ireland|     29100|\n",
      "|       Israel|      1200|\n",
      "|      Iceland|      1200|\n",
      "|  South Korea|      1200|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").sum(\"Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 List countries by their total Price in descending order.\n",
    "\n",
    "Hint: Use orderBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             Country|sum(Price)|\n",
      "+--------------------+----------+\n",
      "|       United States|    350350|\n",
      "|      United Kingdom|     63600|\n",
      "|              Canada|     42000|\n",
      "|              France|     30300|\n",
      "|             Ireland|     29100|\n",
      "|           Australia|     22800|\n",
      "|             Germany|     22800|\n",
      "|         Switzerland|     19200|\n",
      "|         Netherlands|     14400|\n",
      "|              Norway|     12000|\n",
      "|              Brazil|      8700|\n",
      "|             Denmark|      8400|\n",
      "|              Sweden|      8400|\n",
      "|        South Africa|      3600|\n",
      "|             Belgium|      3600|\n",
      "|             Austria|      3600|\n",
      "|United Arab Emirates|      3600|\n",
      "|               Malta|      3600|\n",
      "|               India|      2400|\n",
      "|               Italy|      2400|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").sum(\"Price\").orderBy(\"sum(Price)\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 Now load a second table 'countries'\n",
    "http://www.cse.ust.hk/msbd5003/data/countries.csv \n",
    "\n",
    "df2 = spark.read.csv('countries.csv', header=True, inferSchema=True)\n",
    "Redo Question 3, but replace the country names by their IDs.\n",
    "\n",
    "Hint: Use join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+\n",
      "|       Country| ID|\n",
      "+--------------+---+\n",
      "|United Kingdom|  1|\n",
      "| United States|  2|\n",
      "|     Australia|  3|\n",
      "|        Israel|  4|\n",
      "|        France|  5|\n",
      "|   Netherlands|  6|\n",
      "|       Ireland|  7|\n",
      "|        Canada|  8|\n",
      "|         India|  9|\n",
      "|  South Africa| 10|\n",
      "|       Finland| 11|\n",
      "|   Switzerland| 12|\n",
      "|       Denmark| 13|\n",
      "|       Belgium| 14|\n",
      "|        Sweden| 15|\n",
      "|        Norway| 16|\n",
      "|    Luxembourg| 17|\n",
      "|         Italy| 18|\n",
      "|       Germany| 19|\n",
      "|       Moldova| 20|\n",
      "+--------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv('./data/countries.csv', header=True, inferSchema=True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|      Country|sum(Price)|\n",
      "+-------------+----------+\n",
      "|       Sweden|      8400|\n",
      "|       Jersey|      1200|\n",
      "|     Malaysia|      1200|\n",
      "|       Turkey|      2400|\n",
      "|      Germany|     22800|\n",
      "|       France|     30300|\n",
      "|      Belgium|      3600|\n",
      "|      Finland|      1200|\n",
      "|United States|    350350|\n",
      "|        India|      2400|\n",
      "|       Kuwait|      1200|\n",
      "|        Malta|      3600|\n",
      "|        Italy|      2400|\n",
      "|       Norway|     12000|\n",
      "|        Spain|      2400|\n",
      "|      Denmark|      8400|\n",
      "|      Ireland|     29100|\n",
      "|       Israel|      1200|\n",
      "|      Iceland|      1200|\n",
      "|  South Korea|      1200|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").sum(\"Price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID|sum(Price)|\n",
      "+---+----------+\n",
      "| 31|      1200|\n",
      "| 34|      2400|\n",
      "| 28|      3600|\n",
      "| 26|      3600|\n",
      "| 27|      1200|\n",
      "| 12|     19200|\n",
      "| 22|      3600|\n",
      "|  1|     63600|\n",
      "| 13|      8400|\n",
      "|  6|     14400|\n",
      "| 16|     12000|\n",
      "|  3|     22800|\n",
      "| 20|      1200|\n",
      "|  5|     30300|\n",
      "| 19|     22800|\n",
      "| 15|      8400|\n",
      "|  9|      2400|\n",
      "| 17|      1200|\n",
      "|  4|      1200|\n",
      "|  8|     42000|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = df2.join(df, df2.Country == df.Country)\n",
    "joined.groupBy(\"ID\").sum(\"Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 Rewrite the PageRank example using DataFrame API.  \n",
    "Here is a skeleton of the code.  Your job is to fill in the missing part.  The data files can be downloaded at:\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/pagerank_data.txt\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/dblp.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|src|        rank|\n",
      "+---+------------+\n",
      "|  1|1.3330078125|\n",
      "|  4|         1.0|\n",
      "|  3|         1.0|\n",
      "|  2|0.6669921875|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "lines = spark.read.text(\"./data/pagerank_data.txt\")\n",
    "# You can also test your program on the follow larger data set:\n",
    "#lines = spark.read.text(\"./data/dblp.in\")\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "outdegrees = links.groupBy('src').count()\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank'))\n",
    "\n",
    "def test(u, r):\n",
    "    return u/r\n",
    "\n",
    "for iteration in range(numOfIterations):    \n",
    "    t = links.join(outdegrees, 'src')\\\n",
    "    .select('dst', 'src', 'count').join(ranks, 'src')\n",
    "    t = t.select('*', test(t['rank'], t['count']).alias('slen'))\n",
    "    \n",
    "    t = t.withColumnRenamed('dst', 'dst1').groupBy('dst1').sum('slen')\n",
    "    t = t.withColumnRenamed('dst1', 'src')\n",
    "    t = t.withColumnRenamed('sum(slen)', 'rank')\n",
    "    ranks = t\n",
    "\n",
    "ranks.orderBy(desc('rank')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  1|  3|\n",
      "|  2|  3|\n",
      "|  3|  4|\n",
      "|  4|  1|\n",
      "|  2|  1|\n",
      "+---+---+\n",
      "\n",
      "+---+-----+\n",
      "|src|count|\n",
      "+---+-----+\n",
      "|  3|    1|\n",
      "|  1|    2|\n",
      "|  4|    1|\n",
      "|  2|    2|\n",
      "+---+-----+\n",
      "\n",
      "+---+----+\n",
      "|src|rank|\n",
      "+---+----+\n",
      "|  3|   1|\n",
      "|  1|   1|\n",
      "|  4|   1|\n",
      "|  2|   1|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links.show()\n",
    "outdegrees.show()\n",
    "ranks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There is a bug in the current SparkSQL implementation: The groupBy (followed by some aggregation) method sometimes fails to group all rows with the same key.  A temporary workaround is the following:\n",
    "\n",
    "Suppose you want to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('A').sum('B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it fails to produce the desired result, try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumnRenamed('A', 'A1').groupBy('A1').sum('B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reported this bug to the Spark developers and the issue is currently under investigation:\n",
    "\n",
    "https://issues.apache.org/jira/browse/SPARK-20169 (Links to an external site.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
